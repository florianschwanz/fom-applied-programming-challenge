{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Applied Programming Coding Challenge #1\n",
    "\n",
    "![title](../img/photo-1533788179956-82e8a027c962.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load data\n",
    "\n",
    "```HINT``` Put column names into variables in order to benefit from code completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Original columns\n",
    "col_instant='instant'\n",
    "col_datetime='datetime'\n",
    "col_season='season'\n",
    "col_year='year'\n",
    "col_month='month'\n",
    "col_hour='hour'\n",
    "col_holiday='holiday'\n",
    "col_weekday='weekday'\n",
    "col_workingday='workingday'\n",
    "col_weather_situation='weather_situation'\n",
    "col_temperature='temperature'\n",
    "col_apparent_temperature='apparent_temperature'\n",
    "col_humidity='humidity'\n",
    "col_windspeed='windspeed'\n",
    "\n",
    "# Target columns\n",
    "col_casual='casual'\n",
    "col_registered='registered'\n",
    "col_count='count'\n",
    "\n",
    "# Calculated columns\n",
    "col_days_since_start='days_since_start'\n",
    "\n",
    "# Raw column\n",
    "col_temperature_raw='temperature_raw'\n",
    "col_apparent_temperature_raw='apparent_temperature_raw'\n",
    "col_humidity_raw='humidity_raw'\n",
    "col_windspeed_raw='windspeed_raw'\n",
    "col_temperature_raw_rounded='temperature_raw_rounded'\n",
    "col_days_since_start_raw='days_since_start_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data sets\n",
    "data_set_day = \"day\"\n",
    "data_set_hour = \"hour\"\n",
    "\n",
    "class DataSet:\n",
    "    \"\"\"Contains data, model and predictions of a data set\"\"\"\n",
    "    file_name = \"\"\n",
    "    attribute_names = []\n",
    "    columns_categorical = []\n",
    "    data_frame = {}\n",
    "    variants = {}\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "\n",
    "# Define data sets\n",
    "data_sets = {\n",
    "    data_set_day: DataSet(\"../data/bike-sharing-dataset/day.csv\"),\n",
    "    data_set_hour: DataSet(\"../data/bike-sharing-dataset/hour.csv\")\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define attribute names\n",
    "data_sets[data_set_day].attribute_names = [col_instant, col_datetime, col_season, col_year, col_month, col_holiday, \n",
    "                                           col_weekday, col_workingday, col_weather_situation, col_temperature, \n",
    "                                           col_apparent_temperature, col_humidity, col_windspeed, col_casual, \n",
    "                                           col_registered, col_count]\n",
    "data_sets[data_set_hour].attribute_names = [col_instant, col_datetime, col_season, col_year, col_month, col_hour, \n",
    "                                            col_holiday, col_weekday, col_workingday, col_weather_situation, \n",
    "                                            col_temperature, col_apparent_temperature, col_humidity, col_windspeed, col_casual, col_registered, col_count]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Read csv file\n",
    "    data_set.data_frame = pd.read_csv(data_set.file_name, skiprows=1, names=data_set.attribute_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Understand data\n",
    "\n",
    "### 2.1 Show basic facts\n",
    "\n",
    "* Show data types of features\n",
    "* Make sure there are no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    data_set.data_frame.info()\n",
    "    data_set.data_frame.isnull().sum()\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Set data types"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define categorical attributes\n",
    "data_sets[data_set_day].columns_categorical = [col_season, col_year, col_month, col_holiday, col_weekday, \n",
    "                                               col_workingday, col_weather_situation]\n",
    "data_sets[data_set_hour].columns_categorical = [col_season, col_year, col_month, col_hour, col_holiday, col_weekday, \n",
    "                                                col_workingday, col_weather_situation]\n",
    "\n",
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Convert columns to date time\n",
    "    data_set.data_frame[col_datetime] = pd.to_datetime(data_set.data_frame[col_datetime])\n",
    "\n",
    "    # Convert columns to category\n",
    "    for column in data_set.columns_categorical:\n",
    "        data_set.data_frame[column]=data_set.data_frame[column].astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize raw data\n",
    "\n",
    "* This step is to make sure data is plausible\n",
    "* Unfortunately many features in the data sets are already normalized\n",
    "* For better visualization they need to be de-normalized first (using extreme values from data set's readme file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extreme temperature values\n",
    "temperature_min=-8\n",
    "temperature_max=39\n",
    "# Extreme apparent temperature values\n",
    "apparent_temperature_min=-16\n",
    "apparent_temperature_max=50\n",
    "# Extreme humidity value\n",
    "humidity_max=100\n",
    "# Extreme wind speed value\n",
    "windspeed_max=67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define columns containing raw-values\n",
    "columns_raw_values = [col_temperature_raw, col_apparent_temperature_raw, col_humidity_raw, col_windspeed_raw, \n",
    "                      col_temperature_raw_rounded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Restore raw values for day data frame\n",
    "    data_set.data_frame = data_set.data_frame.assign(temperature_raw=data_set.data_frame[col_temperature] * (temperature_max-temperature_min) + temperature_min)\n",
    "    data_set.data_frame = data_set.data_frame.assign(apparent_temperature_raw=data_set.data_frame[col_apparent_temperature] * (apparent_temperature_max-apparent_temperature_min) + apparent_temperature_min)\n",
    "    data_set.data_frame = data_set.data_frame.assign(humidity_raw=data_set.data_frame[col_humidity] * humidity_max)\n",
    "    data_set.data_frame = data_set.data_frame.assign(windspeed_raw=data_set.data_frame[col_windspeed] * windspeed_max)\n",
    "    \n",
    "    # Round values for better visualization\n",
    "    data_set.data_frame = data_set.data_frame.assign(temperature_raw_rounded=round(data_set.data_frame[col_temperature_raw]/5,0)*5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Show data\n",
    "\n",
    "```WARNING``` DataFrame.head() does not work inside loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Show first few lines\n",
    "    data_set.data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_day].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_hour].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2.2 Plot relation between month and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot contains few peaks (most popular biking months)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_month,col_count,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='point', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2.3 Plot relation between temperature and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot contains one peak (optimal biking temperature)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_temperature_raw_rounded,col_count,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='point', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2.4 Plot relation between weather situation and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot indicates that there are significantly more rentals on day having good weather (clear or misty) \n",
    "  * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "  * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "  * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "  * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Plot trend\n",
    "    sns.catplot(col_weather_situation,col_count,hue=col_year,data=data_set.data_frame, ci=None, kind='strip', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2.4 Plot correlation matrix of some attributes\n",
    "\n",
    "* Expected matrix contains the following correlations\n",
    "  * strong correlation between _temperature_ and _apparent temperature_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    correlationMatrix=data_set.data_frame[[col_temperature, col_apparent_temperature, col_humidity,col_windspeed,\n",
    "                                           col_casual, col_registered]].corr()\n",
    "    mask=np.array(correlationMatrix)\n",
    "    mask[np.tril_indices_from(mask)]=False\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(15,8))\n",
    "    sns.heatmap(correlationMatrix,mask=mask,vmax=0.8,square=True,annot=True,ax=ax)\n",
    "    ax.set_title('Correlation Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.0 Initialize variants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Variants\n",
    "variant_original = \"original\"\n",
    "variant_market = \"market\"\n",
    "\n",
    "class Variant:\n",
    "    \"\"\"Contains data, model and predictions of a variant\"\"\"\n",
    "    color = \"#000\"\n",
    "    data_frame = {}\n",
    "    splitted_data = {}\n",
    "    models = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    def __init__(self, color):\n",
    "        self.color = color"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Assign variants to data frames\n",
    "    data_set.variants = {\n",
    "        variant_original: Variant(color=\"#3fe0e0\"),\n",
    "        variant_market: Variant(color=\"#d4dd80\"),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 One-hot-encode categorical data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # One-hot encode columns \n",
    "    for column in data_set.columns_categorical:\n",
    "        data_set.data_frame = pd.concat([data_set.data_frame,pd.get_dummies(data_set.data_frame[column], prefix=column)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Enhance data\n",
    "\n",
    "* The following variants shall be compared\n",
    "  * _original_ original features\n",
    "  * _market_ original features + time since start of bike sharing campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        # Copy original data frame\n",
    "        variant.data_frame = data_set.data_frame.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.1 Calculate date since start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Calculate days since start\n",
    "    start_date = data_set.variants[variant_market].data_frame[col_datetime].min()\n",
    "    data_set.variants[variant_market].data_frame[col_days_since_start_raw] = \\\n",
    "        data_set.variants[variant_market].data_frame.apply(lambda row: (row[col_datetime] - start_date).days, axis = 1) \n",
    "    \n",
    "    # Normalize days since start\n",
    "    max_days_since_start = data_set.variants[variant_market].data_frame[col_days_since_start_raw].max()\n",
    "    data_set.variants[variant_market].data_frame[col_days_since_start] = \\\n",
    "        data_set.variants[variant_market].data_frame.apply(lambda row: (row[col_days_since_start_raw] / max_days_since_start), axis = 1)\n",
    "    \n",
    "    # Drop raw days since start\n",
    "    data_set.variants[variant_market].data_frame.drop(col_days_since_start_raw, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    correlationMatrix=data_set.variants[variant_market].data_frame[[col_temperature, col_apparent_temperature, \n",
    "                                                                    col_humidity, col_windspeed, col_days_since_start, \n",
    "                                                                    col_count]].corr()\n",
    "    mask=np.array(correlationMatrix)\n",
    "    mask[np.tril_indices_from(mask)]=False\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(15,8))\n",
    "    sns.heatmap(correlationMatrix,mask=mask,vmax=0.8,square=True,annot=True,ax=ax)\n",
    "    ax.set_title('Correlation Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Normalize data\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Drop columns\n",
    "\n",
    "* The following columns need to be dropped\n",
    "  * Indices\n",
    "    * _instant_ since it does not contain any information besides the order\n",
    "  * Columns which cannot be used directly\n",
    "    * _datetime_ which is formatted ```yyyy-mm-dd```\n",
    "  * Columns which are one-hot encoded\n",
    "  * Columns which contain raw values\n",
    "  * Unused target columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        # Drop index and unusable columns\n",
    "        variant.data_frame.drop([col_instant, col_datetime, col_temperature], axis=1, inplace=True)\n",
    "        # Drop one-hot encoded columns\n",
    "        variant.data_frame.drop(data_set.columns_categorical, axis=1, inplace=True)\n",
    "        # Drop raw-value columns\n",
    "        variant.data_frame.drop(columns_raw_values, axis=1, inplace=True)\n",
    "        # Drop unused target columns    \n",
    "        variant.data_frame.drop([col_casual, col_registered], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Split data into _training_ and _test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "    \"\"\"Contains results of a train/test data split\"\"\"\n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        # Define columns\n",
    "        data_bike_day_x = variant.data_frame.drop([col_count], axis=1)\n",
    "        data_bike_day_y = variant.data_frame[[col_count]]\n",
    "        \n",
    "        test_size = 0.3\n",
    "        random_state = 0\n",
    "        \n",
    "        # Split training data and test data\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data_bike_day_x, data_bike_day_y, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Add splitted data \n",
    "        variant.splitted_data = DataSplit(x_train, x_test, y_train, y_test)\n",
    "        \n",
    "        # Show splitted data\n",
    "        print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5 Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Put models into dictionary\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(min_samples_split=10,max_leaf_nodes=100),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=200),\n",
    "    \"Lasso\": Lasso(alpha=0.2),\n",
    "    \"Elastic Net\": ElasticNet(alpha=0.2, random_state=0),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Support Vector Regression (linear)\": SVR(kernel='linear', gamma='scale', C=1.0, epsilon=0.2),\n",
    "    \"Support Vector Regression (rbf)\": SVR(kernel='rbf', gamma='scale', C=1.0, epsilon=0.2)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "    \n",
    "        # Iterate over models\n",
    "        for model_name, model in models.items():\n",
    "            \n",
    "            # Attach model to variant\n",
    "            variant.models[model_name] = model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trainAndEvaluateDataSet(data_set_name, data_set):\n",
    "    \"\"\"Trains and evaluates a given data set\"\"\"\n",
    "    \n",
    "    variant_index = 0\n",
    "    y_pos = np.arange(len(models.keys()))\n",
    "\n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        model_index = 0\n",
    "        model_r2_scores = []\n",
    "        \n",
    "        # Iterate over models in variant \n",
    "        for model_name, model in variant.models.items():\n",
    "            \n",
    "            # print(data_set_name + \" / \" + variant_name + \" / \" + model_name)\n",
    "            \n",
    "            # Fit model and create prediction\n",
    "            model.fit(variant.splitted_data.x_train, variant.splitted_data.y_train)\n",
    "            \n",
    "            # Make prediction\n",
    "            predicted_values = model.predict(variant.splitted_data.x_test)\n",
    "            expected_values = variant.splitted_data.y_test\n",
    "            \n",
    "            # Calculate scores\n",
    "            score_explained_variance_score = explained_variance_score(expected_values, predicted_values)\n",
    "            score_max_error = max_error(expected_values, predicted_values)\n",
    "            score_mean_absolute_error = mean_absolute_error(expected_values, predicted_values)\n",
    "            score_mean_squared_error = mean_squared_error(expected_values, predicted_values)\n",
    "            score_median_absolute_error = median_absolute_error(expected_values, predicted_values)\n",
    "            score_r2_score = r2_score(expected_values, predicted_values)\n",
    "            \n",
    "            # print(\"explained variance score\", round(score_explained_variance_score, 2))\n",
    "            # print(\"               max error\", round(score_max_error, 2))\n",
    "            # print(\"     mean absolute error\", round(score_mean_absolute_error, 2))\n",
    "            # print(\"      mean squared error\", round(score_mean_squared_error, 2))\n",
    "            # print(\"   median absolute error\", round(score_median_absolute_error, 2))\n",
    "            # print(\"                r2 score\", round(score_r2_score, 2))\n",
    "            \n",
    "            print(data_set_name + \" / \" + variant_name + \" / \" + model_name + \" > \" + str(round(score_r2_score, 2)))\n",
    "\n",
    "            model_r2_scores.insert(model_index, score_r2_score)\n",
    "            model_index += 1\n",
    "        \n",
    "        plt.barh(y_pos + (variant_index*0.5), model_r2_scores, 0.4, align='center', alpha=0.5, color=variant.color)\n",
    "        variant_index += 1\n",
    "    \n",
    "    plt.xlabel('R2 Score')\n",
    "    plt.yticks(y_pos, models.keys())\n",
    "    plt.title('Model performance')\n",
    "    plt.show()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and evaluate data set 'day'\n",
    "trainAndEvaluateDataSet(data_set_day, data_sets[data_set_day])   "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and evaluate data set 'hour'\n",
    "trainAndEvaluateDataSet(data_set_hour, data_sets[data_set_hour])   \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}