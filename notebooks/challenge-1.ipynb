{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Applied Programming Coding Challenge #1\n",
    "\n",
    "![title](../img/photo-1533788179956-82e8a027c962.jpg)\n",
    "\n",
    "# General Information\n",
    "\n",
    "## Data Set and Features\n",
    "\n",
    "The project [Bike Sharing Dataset](https://www.kaggle.com/lakshmi25npathi/bike-sharing-dataset) contains two data sets\n",
    "* day.csv\n",
    "* hour.csv\n",
    "\n",
    "Both shall be used to predict demand of bikes using regression.\n",
    "  \n",
    "There are 16 (17 respectively) features in each of the two data sets\n",
    "* ```instant``` Unique ID of a record\n",
    "* Features describing time\n",
    "  * ```dteday``` Date, e.g. 01.01.2011\n",
    "  * ```season``` Numerically encoded season, e.g. 1 (=spring)\n",
    "  * ```yr``` Year after start (Jan 1 2011), e.g. 0\n",
    "  * ```mnth``` Month of the year, e.g. 1 (=January)\n",
    "  * ```hour``` Hour of the day\n",
    "  * ```holiday``` Indicator whether the day is a public holiday, e.g. 0 (=no)\n",
    "  * ```weekday``` Numerically encoded weekday of the date, e.g. 0 (=Sunday)\n",
    "  * ```workingday```  Indicator whether the day is a working day, e.g. 0 (=no)\n",
    "* Features describing weather\n",
    "  * ```weathersit``` Numerically encoded weather situation, e.g.\n",
    "    * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "  * ```temp``` Normalized temperature in Celsius\n",
    "  * ```atemp``` Normalized _feeling_ temperature in Celsius\n",
    "  * ```hum``` Normalized humidity\n",
    "  * ```windspeed``` Normalized wind speed\n",
    "* Features describing rented bikes\n",
    "  * ```casual``` Number of casual users\n",
    "  * ```registered``` Number of registered users\n",
    "  * ```cnt``` Combined number of casual and registered users\n",
    "\n",
    "Some features are already normalized\n",
    "* ```temp```\n",
    "* ```atemp```\n",
    "* ```hum```\n",
    "* ```windspeed```\n",
    "\n",
    "## Overall structure and Python classes being used\n",
    "\n",
    "In the course of this notebook different setups will be trained, evaluated and compared with each other\n",
    "\n",
    "* 2 **data sets** (```day``` and ```hour```)\n",
    "  * 4 **variants** (```original```, ```market```, ```history```, ```reduced```)\n",
    "    * 10 **folds**\n",
    "    * 7 **models** (```Linear Regression```, ```Decision Tree Regressor```, ```Random Forest Regressor```, \n",
    "    ```Gradient Boosting Regressor```, ```Lasso```, ```Elastic Net```, ```Ridge```)\n",
    "      * 1 **metric** (```RÂ² score```)\n",
    "\n",
    "In order to handle the different setups effectively therefore the following classes will be used\n",
    "\n",
    "* ```DataSet``` representing the two data sets _day_ and _hour_\n",
    "* ```Variant``` representing different variants two be compared (see chapter _Hypotheses_)\n",
    "* ```DataSplit``` storing x_train, x_test, y_train, y_test of a train/test data split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Load data\n",
    "\n",
    "```TIP FOR DEVS``` Put column names into variables in order to benefit from code completion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Original columns\n",
    "col_instant = 'instant'\n",
    "col_datetime = 'datetime'\n",
    "col_season = 'season'\n",
    "col_year = 'year'\n",
    "col_month = 'month'\n",
    "col_hour = 'hour'\n",
    "col_holiday = 'holiday'\n",
    "col_weekday = 'weekday'\n",
    "col_workingday = 'workingday'\n",
    "col_weather_situation = 'weather_situation'\n",
    "col_temperature = 'temperature'\n",
    "col_apparent_temperature = 'apparent_temperature'\n",
    "col_humidity = 'humidity'\n",
    "col_windspeed = 'windspeed'\n",
    "\n",
    "# Historic columns\n",
    "col_weather_situation_hist = 'weather_situation_hist'\n",
    "col_temperature_hist = 'temperature_hist'\n",
    "col_apparent_temperature_hist = 'apparent_temperature_hist'\n",
    "col_humidity_hist = 'humidity_hist'\n",
    "col_windspeed_hist = 'windspeed_hist'\n",
    "\n",
    "# Target columns\n",
    "col_casual = 'casual'\n",
    "col_registered = 'registered'\n",
    "col_count = 'count'\n",
    "\n",
    "# Calculated columns\n",
    "col_days_since_start = 'days_since_start'\n",
    "\n",
    "# Raw column\n",
    "col_temperature_raw = 'temperature_raw'\n",
    "col_apparent_temperature_raw = 'apparent_temperature_raw'\n",
    "col_humidity_raw = 'humidity_raw'\n",
    "col_windspeed_raw = 'windspeed_raw'\n",
    "col_temperature_raw_rounded = 'temperature_raw_rounded'\n",
    "col_days_since_start_raw = 'days_since_start_raw'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```TIP FOR DEVS``` Use random_state in order to ensure reproducible results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random state to generate reproducible results \n",
    "random_state = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data sets\n",
    "data_set_day = \"day\"\n",
    "data_set_hour = \"hour\"\n",
    "\n",
    "class DataSet:\n",
    "    \"\"\"Contains data, model and predictions of a data set\"\"\"\n",
    "    file_name = \"\"\n",
    "    attribute_names = []\n",
    "    columns_categorical = []\n",
    "    data_frame = {}\n",
    "    variants = {}\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "\n",
    "# Define data sets\n",
    "data_sets = {\n",
    "    # data_set_day: DataSet(\"../data/bike-sharing-dataset/day.csv\"),\n",
    "    # data_set_hour: DataSet(\"../data/bike-sharing-dataset/hour.csv\")\n",
    "    data_set_day: DataSet(\"https://raw.githubusercontent.com/florianschwanz/fom-applied-programming-challenge/master/data/bike-sharing-dataset/day.csv\"),\n",
    "    data_set_hour: DataSet(\"https://raw.githubusercontent.com/florianschwanz/fom-applied-programming-challenge/master/data/bike-sharing-dataset/hour.csv\")\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define attribute names\n",
    "data_sets[data_set_day].attribute_names = [col_instant, col_datetime, col_season, col_year, col_month, col_holiday, \n",
    "                                           col_weekday, col_workingday, col_weather_situation, col_temperature, \n",
    "                                           col_apparent_temperature, col_humidity, col_windspeed, col_casual, \n",
    "                                           col_registered, col_count]\n",
    "data_sets[data_set_hour].attribute_names = [col_instant, col_datetime, col_season, col_year, col_month, col_hour, \n",
    "                                            col_holiday, col_weekday, col_workingday, col_weather_situation, \n",
    "                                            col_temperature, col_apparent_temperature, col_humidity, col_windspeed, \n",
    "                                            col_casual, col_registered, col_count]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Read csv file\n",
    "    data_set.data_frame = pd.read_csv(data_set.file_name, skiprows=1, names=data_set.attribute_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Understand data\n",
    "\n",
    "* Make sure data is plausible\n",
    "\n",
    "### 2.1 Show basic facts\n",
    "\n",
    "* Show data types of features\n",
    "* Make sure there are no null values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    data_set.data_frame.info()\n",
    "    data_set.data_frame.isnull().sum()\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Set data types\n",
    "\n",
    "Data types are assigned to features in order to make it easier to use them e.g. for calculations in further steps.\n",
    "This includes\n",
    "* date time data\n",
    "* categorical data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define categorical attributes\n",
    "data_sets[data_set_day].columns_categorical = [col_season, col_year, col_month, col_holiday, col_weekday,\n",
    "                                               col_workingday, col_weather_situation]\n",
    "data_sets[data_set_hour].columns_categorical = [col_season, col_year, col_month, col_hour, col_holiday, col_weekday,\n",
    "                                                col_workingday, col_weather_situation]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Convert columns to date time\n",
    "    data_set.data_frame[col_datetime] = pd.to_datetime(data_set.data_frame[col_datetime])\n",
    "\n",
    "    # Convert columns to category\n",
    "    for column in data_set.columns_categorical:\n",
    "        data_set.data_frame[column]=data_set.data_frame[column].astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Restore raw data\n",
    "\n",
    "* Some features in the data sets are already normalized\n",
    "* For better visualization they need to be de-normalized first (using extreme values from data set's readme file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extreme temperature values\n",
    "temperature_min=-8\n",
    "temperature_max=39\n",
    "# Extreme apparent temperature values\n",
    "apparent_temperature_min=-16\n",
    "apparent_temperature_max=50\n",
    "# Extreme humidity value\n",
    "humidity_max=100\n",
    "# Extreme wind speed value\n",
    "windspeed_max=67"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define columns containing raw-values\n",
    "columns_raw_values = [col_temperature_raw, col_apparent_temperature_raw, col_humidity_raw, col_windspeed_raw,\n",
    "                      col_temperature_raw_rounded]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Restore raw values for day data frame\n",
    "    data_set.data_frame = data_set.data_frame.assign(temperature_raw=data_set.data_frame[col_temperature] * (temperature_max-temperature_min) + temperature_min)\n",
    "    data_set.data_frame = data_set.data_frame.assign(apparent_temperature_raw=data_set.data_frame[col_apparent_temperature] * (apparent_temperature_max-apparent_temperature_min) + apparent_temperature_min)\n",
    "    data_set.data_frame = data_set.data_frame.assign(humidity_raw=data_set.data_frame[col_humidity] * humidity_max)\n",
    "    data_set.data_frame = data_set.data_frame.assign(windspeed_raw=data_set.data_frame[col_windspeed] * windspeed_max)\n",
    "\n",
    "    # Round values for better visualization\n",
    "    data_set.data_frame = data_set.data_frame.assign(temperature_raw_rounded=round(data_set.data_frame[col_temperature_raw]/5,0)*5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Show data\n",
    "\n",
    "```WARNING``` DataFrame.head() does not work inside loops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Show first few lines\n",
    "    data_set.data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_day].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_hour].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 Plot relation between month and number of rented bikes\n",
    "\n",
    "* Expected plot contains few peaks (most popular biking months)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_month,col_count,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='point', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.6 Plot relation between temperature and number of rented bikes\n",
    "\n",
    "* Expected plot contains one peak (optimal biking temperature)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_temperature_raw_rounded,col_count,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='point', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.7 Plot relation between weather situation and number of rented bikes\n",
    "\n",
    "* Expected plot indicates that there are significantly more rentals on day having good weather (clear or misty)\n",
    "  * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "  * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "  * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "  * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_weather_situation,col_count,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='strip', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.8 Plot relation between hour of the day and number of rented bikes\n",
    "\n",
    "* Expected plot indicates that more bikes are rented by **casual casual** during mid day\n",
    "  * Probably when tourists and people on holidays move through the city\n",
    "* Expected plot indicates that more bikes are rented by **registered users** during rush hours\n",
    "  * Probably there are two peeks when people go to work / go back home"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_hour,col_casual,hue=col_year,data=data_sets[data_set_hour].data_frame, ci=None, kind='point', palette='rainbow')\n",
    "sns.catplot(col_hour,col_registered,hue=col_year,data=data_sets[data_set_hour].data_frame, ci=None, kind='point', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.9 Plot relation between day of the week and number of rented bikes\n",
    "\n",
    "* Expected plot indicates that more bikes are rented by **casual users** on weekends\n",
    "* Expected plot indicates that more bikes are rented by **registered users** on week days"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_weekday,col_casual,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='point', palette='rainbow')\n",
    "sns.catplot(col_weekday,col_registered,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='point', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.10 Plot correlation matrix of some attributes\n",
    "\n",
    "* Expected matrix contains the following correlations\n",
    "  * strong correlation between _temperature_ and _apparent temperature_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Plot correlation matrix\n",
    "    correlationMatrix=data_set.data_frame[[col_temperature, col_apparent_temperature, col_humidity,col_windspeed,\n",
    "                                           col_casual, col_registered]].corr()\n",
    "    mask=np.array(correlationMatrix)\n",
    "    mask[np.tril_indices_from(mask)]=False\n",
    "\n",
    "    fig,ax=plt.subplots(figsize=(15,8))\n",
    "    sns.heatmap(correlationMatrix,mask=mask,vmax=0.8,square=True,annot=True,ax=ax)\n",
    "    ax.set_title('Correlation Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Hypotheses\n",
    "\n",
    "Based on the basic understanding of the data sets two hypotheses shall be made\n",
    "\n",
    "In the course of this notebook the data sets will be extended by additional information\n",
    "* days since start of the bike sharing campaign\n",
    "* weather conditions of previous period\n",
    "\n",
    "## 3.1 Hypothesis A <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #3fe0e0;\">time in the market</span>\n",
    "\n",
    "There is a significant difference in the number of rented bikes in the two years being contained in the data set. \n",
    "Therefore it can be assumed that the raising popularity correlates with the number of rented bikes.\n",
    "In order to include this assumption a synthentic feature ```days since start``` shall be used.\n",
    "\n",
    "*Hypothesis* Taking into account the number of days since the bike sharing campaign has started will increase the \n",
    "performance of the model.\n",
    "\n",
    "## 3.2 Hypothesis B  <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #3fb5e0;\">historic weather data</span>\n",
    "\n",
    "Different aspects of the weather affect how many bikes are rented. In most cases there may be time between _deciding to \n",
    "use a shared bike_ and _actually using it_. Therefore also the weather conditions of the time period \n",
    "_before_ a bike is rented may be relevant. In order to achieve this some synthetic features such as \n",
    "```weather_situation_hist```, ```temperature_hist```, ```apparent_temperature_hist```, ```humidity_hist```, \n",
    "```windspeed_hist``` shall be included which reflect the weather situation of the previous row.\n",
    "\n",
    "*Hypothesis* Taking into account the weather conditions of the previous period will increase the \n",
    "performance of the model.\n",
    "\n",
    "## 3.3 Hypothesis C <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #ff8080;\">dimensionality reduction</span>\n",
    "\n",
    "Features ```temperature``` and ```apparent_temperature``` strongly correlate with each other. It is to be assumed that \n",
    "dropping one of the two does not reduce model performance.\n",
    "\n",
    "*Hypothesis* Dropping _apparent temperature_ does not decrease performance of the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 Preprocess data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Initialize variants\n",
    "\n",
    "Three variants will be trained and evaluated\n",
    "* <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #e9edbf;\">original</span> original data frame\n",
    "* <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #3fe0e0;\">market</span> original data frame + days since start\n",
    "* <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #3fb5e0;\">history</span> original data frame + history weather conditions\n",
    "* <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #ff8080;\">reduced</span> original data frame + manual dimensionality reduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Variants\n",
    "variant_original = \"original\"\n",
    "variant_market = \"market\"\n",
    "variant_history = \"history\"\n",
    "variant_reduced = \"reduced\"\n",
    "\n",
    "class Variant:\n",
    "    \"\"\"Contains data, model and predictions of a variant\"\"\"\n",
    "    color = \"#000\"\n",
    "    data_frame = {}\n",
    "    folds = {}\n",
    "    models = {}\n",
    "    \n",
    "    def __init__(self, color):\n",
    "        self.color = color"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Assign variants to data frames\n",
    "    data_set.variants = {\n",
    "        variant_original: Variant(color=\"#d4dd80\"),\n",
    "        variant_market: Variant(color=\"#3fe0e0\"),\n",
    "        variant_history: Variant(color=\"#3fb5e0\"),\n",
    "        variant_reduced: Variant(color=\"red\"),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 One-hot-encode categorical data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # One-hot encode columns \n",
    "    for column in data_set.columns_categorical:\n",
    "        data_set.data_frame = pd.concat([data_set.data_frame,pd.get_dummies(data_set.data_frame[column], prefix=column)],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 Enhance data\n",
    "\n",
    "* The following variants shall be compared\n",
    "  * _original_ original features\n",
    "  * _market_ original features + time since start of bike sharing campaign"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        # Copy original data frame\n",
    "        variant.data_frame = data_set.data_frame.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.1 Variant <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #3fe0e0;\">market</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Calculate days since start\n",
    "    start_date = data_set.variants[variant_market].data_frame[col_datetime].min()\n",
    "    data_set.variants[variant_market].data_frame[col_days_since_start_raw] = \\\n",
    "        data_set.variants[variant_market].data_frame.apply(lambda row: (row[col_datetime] - start_date).days, axis = 1) \n",
    "    \n",
    "    # Normalize days since start\n",
    "    max_days_since_start = data_set.variants[variant_market].data_frame[col_days_since_start_raw].max()\n",
    "    data_set.variants[variant_market].data_frame[col_days_since_start] = \\\n",
    "        data_set.variants[variant_market].data_frame.apply(lambda row: (row[col_days_since_start_raw] / max_days_since_start), axis = 1)\n",
    "    \n",
    "    # Drop raw days since start\n",
    "    data_set.variants[variant_market].data_frame.drop(col_days_since_start_raw, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_day].variants[variant_market].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    correlationMatrix=data_set.variants[variant_market].data_frame[[col_temperature, col_apparent_temperature, \n",
    "                                                                    col_humidity, col_windspeed, col_days_since_start, \n",
    "                                                                    col_count]].corr()\n",
    "    mask=np.array(correlationMatrix)\n",
    "    mask[np.tril_indices_from(mask)]=False\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(15,8))\n",
    "    sns.heatmap(correlationMatrix,mask=mask,vmax=0.8,square=True,annot=True,ax=ax)\n",
    "    ax.set_title('Correlation Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.2 Variant <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #3fb5e0;\">history</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Days / hours to look backwards\n",
    "    historical_period = 1\n",
    "    \n",
    "    # Copy columns into hist columns and shift them by X\n",
    "    data_set.variants[variant_history].data_frame[col_weather_situation_hist] = data_set.variants[variant_history].data_frame[col_weather_situation].shift(historical_period)\n",
    "    data_set.variants[variant_history].data_frame[col_temperature_hist] = data_set.variants[variant_history].data_frame[col_temperature].shift(historical_period)\n",
    "    data_set.variants[variant_history].data_frame[col_apparent_temperature_hist] = data_set.variants[variant_history].data_frame[col_apparent_temperature].shift(historical_period)\n",
    "    data_set.variants[variant_history].data_frame[col_humidity_hist] = data_set.variants[variant_history].data_frame[col_humidity].shift(historical_period)\n",
    "    data_set.variants[variant_history].data_frame[col_windspeed_hist] = data_set.variants[variant_history].data_frame[col_windspeed].shift(historical_period)\n",
    "    \n",
    "    # Remove row historical_period\n",
    "    data_set.variants[variant_history].data_frame = data_set.variants[variant_history].data_frame.iloc[historical_period:]\n",
    "    \n",
    "    # One-hot encode columns \n",
    "    data_set.variants[variant_history].data_frame = pd.concat([data_set.variants[variant_history].data_frame, pd.get_dummies(data_set.variants[variant_history]\n",
    "                                                                              .data_frame[col_weather_situation_hist], prefix=col_weather_situation_hist)],axis=1)\n",
    "    # Drop one-hot encoded columns\n",
    "    data_set.variants[variant_history].data_frame.drop([col_weather_situation_hist], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_day].variants[variant_history].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.3 Variant <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #ff8080;\">reduced</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # data_set.variants[variant_reduced].data_frame.drop(list(data_set.variants[variant_reduced].data_frame.filter(regex=col_season + \"_\")), axis=1, inplace=True)\n",
    "    data_set.variants[variant_reduced].data_frame.drop(list(data_set.variants[variant_reduced].data_frame.filter(regex=\"^\" + col_apparent_temperature + \"$\")), axis=1, inplace=True)\n",
    "    # data_set.variants[variant_reduced].data_frame.drop(list(data_set.variants[variant_reduced].data_frame.filter(regex=col_weekday + \"_\")), axis=1, inplace=True)\n",
    "    # data_set.variants[variant_reduced].data_frame.drop(list(data_set.variants[variant_reduced].data_frame.filter(regex=col_month + \"_\")), axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_day].variants[variant_reduced].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 Normalize data\n",
    "\n",
    "Nothing to be done here since features are already normalized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 Drop columns\n",
    "\n",
    "* The following columns need to be dropped\n",
    "  * Indices\n",
    "    * _instant_ since it does not contain any information besides the order\n",
    "  * Columns which cannot be used directly\n",
    "    * _datetime_ which is formatted ```yyyy-mm-dd```\n",
    "  * Columns which are one-hot encoded\n",
    "  * Columns which contain raw values\n",
    "  * Unused target columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        # Drop index and unusable columns\n",
    "        variant.data_frame.drop([col_instant, col_datetime], axis=1, inplace=True)\n",
    "        # Drop one-hot encoded columns\n",
    "        variant.data_frame.drop(data_set.columns_categorical, axis=1, inplace=True)\n",
    "        # Drop raw-value columns\n",
    "        variant.data_frame.drop(columns_raw_values, axis=1, inplace=True)\n",
    "        # Drop unused target columns    \n",
    "        variant.data_frame.drop([col_casual, col_registered], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Split data into _training_ and _test_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "    \"\"\"Contains results of a train/test data split\"\"\"\n",
    "    def __init__(self, _x_train, _x_test, _y_train, _y_test):\n",
    "        self.x_train = _x_train\n",
    "        self.x_test = _x_test\n",
    "        self.y_train = _y_train\n",
    "        self.y_test = _y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of splits\n",
    "n_splits = 10\n",
    "\n",
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        # Define columns\n",
    "        data_bike_x = variant.data_frame.drop([col_count], axis=1)\n",
    "        data_bike_y = variant.data_frame[[col_count]]\n",
    "        \n",
    "        # Random state to generate reproducible results \n",
    "        random_state = 0\n",
    "        \n",
    "        # Initialize KFold\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        \n",
    "        fold_index = 0\n",
    "        variant.folds = {}\n",
    "        \n",
    "        for train_index, test_index in kf.split(data_bike_x):\n",
    "        \n",
    "            x_train, x_test = data_bike_x.iloc[train_index], data_bike_x.iloc[test_index]\n",
    "            y_train, y_test = data_bike_y.iloc[train_index], data_bike_y.iloc[test_index]\n",
    "            \n",
    "            # Add splitted data \n",
    "            variant.folds[fold_index] = DataSplit(x_train, x_test, y_train, y_test)  \n",
    "            \n",
    "            # Show splitted data\n",
    "            # print(\"data set \" + data_set_name + \" / variant \" + variant_name + \" / \" + str(x_train.shape) + \", \" + str(x_test.shape) + \", \" + str(y_train.shape) + \", \" + str(y_test.shape))\n",
    "\n",
    "            fold_index += 1\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Initialize model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of estimators for ensemble learning\n",
    "n_estimators = 200\n",
    "\n",
    "# Random state to generate reproducible results \n",
    "random_state = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Models\n",
    "model_linear_regression = \"Linear Regression\"\n",
    "model_decision_tree_regressor_200 = \"Decision Tree Regressor (200)\"\n",
    "model_gradient_boosting_regressor = \"Gradient Boosting Regressor\"\n",
    "model_random_forest_regressor_200 = \"Random Forest Regressor (200)\"\n",
    "model_random_forest_regressor_hp_optimized = \"Random Forest Regressor (hp-optimized)\"\n",
    "model_lasso_1_0 = \"Lasso (1.0)\"\n",
    "model_elastic_net_1_0 = \"Elastic Net (1.0)\"\n",
    "model_ridge_1_0 =  \"Ridge (1.0)\"\n",
    "\n",
    "# Put models into dictionary\n",
    "models = {\n",
    "    model_linear_regression: LinearRegression(),\n",
    "    model_decision_tree_regressor_200: DecisionTreeRegressor(min_samples_split=10, max_leaf_nodes=200, random_state=random_state),\n",
    "    model_gradient_boosting_regressor: GradientBoostingRegressor(random_state=random_state),\n",
    "    model_random_forest_regressor_200: RandomForestRegressor(n_estimators=n_estimators, random_state=random_state),\n",
    "    model_lasso_1_0: Lasso(alpha=1.0, random_state=random_state),\n",
    "    model_elastic_net_1_0: ElasticNet(alpha=1.0, random_state=random_state),\n",
    "    model_ridge_1_0: Ridge(alpha=1.0, random_state=random_state),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "\n",
    "        # Iterate over models\n",
    "        for model_name, model in models.items():\n",
    "\n",
    "            # Attach model to variant\n",
    "            variant.models[model_name] = model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7 Train and evaluate model\n",
    "\n",
    "## 7.1 Preview performance\n",
    "\n",
    "* Do preliminary investigation to get a first impression on how different models perform\n",
    "* Using only data set ```day``` and variant ```original``` "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_and_evaluate_data_set_preview(_data_set_name, _data_set):\n",
    "    \"\"\"Trains and evaluates a given data set\"\"\"\n",
    "\n",
    "    _variant_name = variant_original\n",
    "    _variant = _data_set.variants[variant_original]\n",
    "\n",
    "    variant_index = 1\n",
    "    y_pos = np.arange(len(models.keys()))\n",
    "    handles = []\n",
    "    \n",
    "    plt.figure(figsize=(10,2.5))\n",
    "\n",
    "    print(_variant_name)\n",
    "    \n",
    "    model_index = 0\n",
    "    metric_r2_score_list = []\n",
    "\n",
    "    # Iterate over models in variant\n",
    "    for _model_name, _model in variant.models.items():\n",
    "\n",
    "        _start = time.time()\n",
    "\n",
    "        # Initialize metric\n",
    "        metric_r2_score = 0\n",
    "\n",
    "        # Iterate over folds in variant\n",
    "        for _fold_name, _fold in variant.folds.items():\n",
    "            \n",
    "            # Fit model and create prediction\n",
    "            _model.fit(_fold.x_train, _fold.y_train)\n",
    "\n",
    "            # Make prediction\n",
    "            predicted_values = _model.predict(_fold.x_test)\n",
    "            expected_values = _fold.y_test\n",
    "\n",
    "            # Calculate scores\n",
    "            metric_r2_score += r2_score(expected_values, predicted_values)\n",
    "\n",
    "        fold_count = len(variant.folds.items())\n",
    "\n",
    "        # Divide scores by number of folds\n",
    "        metric_r2_score /= fold_count\n",
    "\n",
    "        _end = time.time()\n",
    "        print(\"  (\" + str(round(_end-_start, 2)) + \"s)\\t\" + _model_name + \" > \" + str(round(metric_r2_score, 2)))\n",
    "        \n",
    "        metric_r2_score_list.insert(model_index, metric_r2_score)\n",
    "        model_index += 1\n",
    "        \n",
    "    variant_count = 1\n",
    "    \n",
    "    handle = plt.barh(y_pos, metric_r2_score_list, 0.8 / variant_count, align='center', alpha=0.5, color=\"grey\", label=_variant_name)\n",
    "    handles.insert(variant_index, handle)\n",
    "    \n",
    "    plt.xlabel('R2 Score')\n",
    "    plt.yticks(y_pos, models.keys())\n",
    "    plt.title('Model performance')\n",
    "    plt.legend(handles=handles)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and evaluate data set 'day'\n",
    "%time train_and_evaluate_data_set_preview(data_set_day, data_sets[data_set_day])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.2 Hyper-parameter optimization\n",
    "\n",
    "* Do hype-parameter optimization for the model that performed best in preliminary investigation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(_results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(_results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  _results['mean_test_score'][candidate],\n",
    "                  _results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(_results['params'][candidate]))\n",
    "            print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter_search = 500\n",
    "\n",
    "# Random state to generate reproducible results \n",
    "random_state = 0\n",
    "\n",
    "def optimize_parameter_for_random_forest(data_frame, estimator):\n",
    "\n",
    "    # Parameter distribution\n",
    "    param_distribution = {\n",
    "        \"n_estimators\": [n_estimators],\n",
    "        \"criterion\": [\"mse\"], \n",
    "        \"min_samples_split\": sp_randint(2, 20),\n",
    "        \"min_samples_leaf\": sp_randint(2, 20),\n",
    "        \"max_depth\": sp_randint(3, 10)\n",
    "    }\n",
    "    \n",
    "    # Define columns\n",
    "    data_bike_x = data_frame.drop([col_count], axis=1)\n",
    "    data_bike_y = data_frame[[col_count]]\n",
    "    \n",
    "    # Initialize random search\n",
    "    random_search = RandomizedSearchCV(estimator=estimator, \n",
    "                                       param_distributions=param_distribution,\n",
    "                                       n_iter=n_iter_search, cv=5, iid=False, random_state=random_state)\n",
    "    \n",
    "    # Fit random search model\n",
    "    start = time.time()\n",
    "    random_search.fit(data_bike_x, data_bike_y)\n",
    "    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "          \" parameter settings.\" % ((time.time() - start), n_iter_search))\n",
    "    \n",
    "    # Extract results\n",
    "    results = random_search.cv_results_\n",
    "    \n",
    "    # Show results\n",
    "    report(results)\n",
    "    \n",
    "    # Determine best parameters\n",
    "    best_index = np.nonzero(results['rank_test_score'] == 1)[0][0]\n",
    "    best_parameters = results[\"params\"][best_index]\n",
    "    \n",
    "    # Store best parameters\n",
    "    hp_optimized_max_depth = best_parameters['max_depth']\n",
    "    hp_optimized_min_samples_leaf = best_parameters['min_samples_leaf']\n",
    "    hp_optimized_min_samples_split = best_parameters['min_samples_split']\n",
    "    \n",
    "    return RandomForestRegressor(\n",
    "            max_depth=hp_optimized_max_depth, \n",
    "            min_samples_leaf=hp_optimized_min_samples_leaf, \n",
    "            min_samples_split=hp_optimized_min_samples_split, \n",
    "            n_estimators=n_estimators,\n",
    "            random_state=random_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for _data_set_name, _data_set in data_sets.items():\n",
    "\n",
    "    d = _data_set.variants[variant_original].data_frame\n",
    "    m = models[model_random_forest_regressor_200]\n",
    "\n",
    "    # Optimize model\n",
    "    hp_optimized_model_name = model_random_forest_regressor_hp_optimized + \" for \" + _data_set_name\n",
    "    hp_optimized_model = optimize_parameter_for_random_forest(d, m)\n",
    "    \n",
    "    # Add optimized model to general list of models\n",
    "    models[hp_optimized_model_name] = hp_optimized_model\n",
    "    \n",
    "    # Iterate over variants\n",
    "    for _variant_name, _variant in _data_set.variants.items():\n",
    "\n",
    "        # Add optimized model to specific list of models\n",
    "        _variant.models[model_random_forest_regressor_hp_optimized] = hp_optimized_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.3 Train all data sets, variants and models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_and_evaluate_data_set(data_set_name, data_set):\n",
    "    \"\"\"Trains and evaluates a given data set\"\"\"\n",
    "\n",
    "    variant_index = 0\n",
    "    y_pos = np.arange(len(data_set.variants[variant_original].models.keys()))\n",
    "    handles = []\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    # Iterate over variants\n",
    "    for _variant_name, _variant in data_set.variants.items():\n",
    "\n",
    "        print(_variant_name)\n",
    "\n",
    "        model_index = 0\n",
    "        metric_r2_score_list = []\n",
    "\n",
    "        # Iterate over models in variant\n",
    "        for _model_name, _model in _variant.models.items():\n",
    "\n",
    "            _start = time.time()\n",
    "\n",
    "            # Initialize metric\n",
    "            metric_r2_score = 0\n",
    "\n",
    "            # Iterate over folds in variant\n",
    "            for _fold_name, _fold in _variant.folds.items():\n",
    "                \n",
    "                # Fit model and create prediction\n",
    "                _model.fit(_fold.x_train, _fold.y_train)\n",
    "\n",
    "                # Make prediction\n",
    "                predicted_values = _model.predict(_fold.x_test)\n",
    "                expected_values = _fold.y_test\n",
    "\n",
    "                # Calculate scores\n",
    "                metric_r2_score += r2_score(expected_values, predicted_values)\n",
    "\n",
    "            fold_count = len(_variant.folds.items())\n",
    "\n",
    "            # Divide scores by number of folds\n",
    "            metric_r2_score /= fold_count\n",
    "\n",
    "            _end = time.time()\n",
    "            print(\"  (\" + str(round(_end-_start, 2)) + \"s)\\t\" + _model_name + \" > \" + str(round(metric_r2_score, 2)))\n",
    "            \n",
    "            metric_r2_score_list.insert(model_index, metric_r2_score)\n",
    "            model_index += 1\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "        variant_count = len(data_set.variants)\n",
    "    \n",
    "        handle = plt.barh(y_pos + (0.8 * variant_index / variant_count), \n",
    "                          metric_r2_score_list, \n",
    "                          0.8 / variant_count, \n",
    "                          align='center', \n",
    "                          alpha=0.5, \n",
    "                          color=_variant.color, \n",
    "                          label=_variant_name)\n",
    "        handles.insert(variant_index, handle)\n",
    "        variant_index += 1\n",
    "\n",
    "    plt.xlabel('R2 Score')\n",
    "    plt.yticks(y_pos, models.keys())\n",
    "    plt.title('Model performance')\n",
    "    plt.legend(handles=handles)\n",
    "    plt.show()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.3.1 Data Set _day_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and evaluate data set 'day'\n",
    "%time train_and_evaluate_data_set(data_set_day, data_sets[data_set_day])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.3.2 Data Set _hour_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and evaluate data set 'hour'\n",
    "%time train_and_evaluate_data_set(data_set_hour, data_sets[data_set_hour])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8 Conclusion\n",
    "\n",
    "## 8.1 Hypothesis A <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #3fe0e0;\">time in the market</span>\n",
    "\n",
    "In ```day``` data set using synthetic feature _days since start_ did not improve the performance when using\n",
    "* Linear Regression\n",
    "* Decision Tree Regressor (200)\n",
    "* Lasso (1.0)\n",
    "* Elastic Net (1.0)\n",
    "* Ridge (1.0)\n",
    "\n",
    "... and slightly improves the performance when using\n",
    "* Random Forest Regressor (200)\n",
    "* Gradient Boosting Regressor\n",
    "\n",
    "## 8.2 Hypothesis B <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #3fb5e0;\">historic weather data</span>\n",
    "\n",
    "In ```day``` data set using historic data slightly increased performance when using\n",
    "\n",
    "* Lasso (1.0)\n",
    "* Elastic Net (1.0)\n",
    "* Ridge (1.0)\n",
    "\n",
    "... did not affect the performance when using\n",
    "* Linear Regression\n",
    "* Gradient Boosting Regressor \n",
    "* Random Forest Regressor (200)\n",
    "* Decision Tree Regressor (200)\n",
    "\n",
    "## 8.3 Hypothesis C <span style=\"font-style:italic; font-weight: 700; padding: 1px 5px; border-radius: 2px; font-family: monospace; background-color: #ff8080;\">dimensionality reduction</span>\n",
    "\n",
    "In ```day``` data set manually reducing the dimensionality by removing _apparent temperature_ slightly decreases the \n",
    "performance when using\n",
    "\n",
    "* Linear Regression (200)\n",
    "* Decision Tree Regressor (200)\n",
    "* Elastic Net (1.0)\n",
    "\n",
    "... did not affect the performance\n",
    "* Random Forest Regressor\n",
    "* Lasso (1.0)\n",
    "* Rigde (1.0)\n",
    "* GradientBoostingRegressor\n",
    "\n",
    "There is no model which increased its performance compared to the original data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tools and platforms\n",
    "\n",
    "## Dev environment\n",
    "\n",
    "<img style=\"float: left; height: 40px\" src=\"../img/anaconda.jpg\">\n",
    "<img style=\"float: left; height: 40px\" src=\"../img/pycharm.jpg\">\n",
    "<img style=\"float: left; height: 40px\" src=\"../img/git.png\">\n",
    "<div style=\"display: block; clear: both;\"> </div>\n",
    "\n",
    "* [PyCharm Professional Edition for Anaconda](https://www.jetbrains.com/pycharm/promo/anaconda/)\n",
    "  * Debugging and insights\n",
    "  * Code completion\n",
    "  * Git integration\n",
    "* [Git](https://git-scm.com/)\n",
    "  * Versioning\n",
    "  \n",
    "```QUESTION``` How can Jupyter notebooks be versioned without output and execution count?\n",
    "  \n",
    "## Hosting\n",
    "\n",
    "<img style=\"float: left; height: 40px\" src=\"../img/github.png\">\n",
    "<img style=\"float: left; height: 40px\" src=\"../img/colab.png\">\n",
    "<img style=\"float: left; height: 40px\" src=\"../img/binder.png\">\n",
    "<div style=\"display: block; clear: both;\"> </div>\n",
    "\n",
    "* [Github](https://github.com)\n",
    "  * Code hosting\n",
    "* [binder](https://mybinder.org/) and [Colab](https://colab.research.google.com)\n",
    "  * Loading from Github\n",
    "  * Notebook execution\n",
    "  * Compute power\n",
    "  \n",
    "# Follow-up questions\n",
    "\n",
    "```QUESTION``` How is machine learning done in a professional context?\n",
    " \n",
    "* Are Jupyter notebooks primarily used for proof of concepts and demos?\n",
    "* How and where are models usually trained?\n",
    "\n",
    "```QUESTION``` How are trained models used by customers / integrated into existing applications?\n",
    "\n",
    "* How are models persisted, delivered and updated?\n",
    "* Can we discuss the usage of pickle during the lecture?"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}