{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Applied Programming Coding Challenge #1\n",
    "\n",
    "![title](../img/photo-1533788179956-82e8a027c962.jpg)\n",
    "\n",
    "## 0 General Information\n",
    "\n",
    "### 0.1 Data Set and Features\n",
    "\n",
    "The project [Bike Sharing Dataset](https://www.kaggle.com/lakshmi25npathi/bike-sharing-dataset) contains two data sets\n",
    "* day.csv\n",
    "* hour.csv\n",
    "\n",
    "Both shall be used to predict demand of bikes using regression.\n",
    "  \n",
    "There are 16 (17 respectively) features in each of the two data sets\n",
    "* ```instant``` Unique ID of a record\n",
    "* Features describing time\n",
    "  * ```dteday``` Date, e.g. 01.01.2011\n",
    "  * ```season``` Numerically encoded season, e.g. 1 (=spring)\n",
    "  * ```yr``` Year after start (Jan 1 2011), e.g. 0\n",
    "  * ```mnth``` Month of the year, e.g. 1 (=January)\n",
    "  * ```hour``` Hour of the day\n",
    "  * ```holiday``` Indicator whether the day is a public holiday, e.g. 0 (=no)\n",
    "  * ```weekday``` Numerically encoded weekday of the date, e.g. 0 (=Sunday)\n",
    "  * ```workingday```  Indicator whether the day is a working day, e.g. 0 (=no)\n",
    "* Features describing weather\n",
    "  * ```weathersit``` Numerically encoded weather situation, e.g.\n",
    "    * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "  * ```temp``` Normalized temperature in Celsius\n",
    "  * ```atemp``` Normalized _feeling_ temperature in Celsius\n",
    "  * ```hum``` Normalized humidity\n",
    "  * ```windspeed``` Normalized wind speed\n",
    "* Features describing rented bikes\n",
    "  * ```casual``` Number of casual users\n",
    "  * ```registered``` Number of registered users\n",
    "  * ```cnt``` Combined number of casual and registered users\n",
    "\n",
    "Some features are already normalized\n",
    "* ```temp```\n",
    "* ```atemp```\n",
    "* ```hum```\n",
    "* ```windspeed```\n",
    "\n",
    "### 0.2 Overall structure and Python classes being used\n",
    "\n",
    "In the course of this notebook different setups will be trained, evaluated and compared with each other\n",
    "\n",
    "* 2 **data sets** (```day``` and ```hour```)\n",
    "  * 4 **variants** (```original```, ```market```, ```history```, ```reduced```)\n",
    "    * 5 **folds**\n",
    "    * 9 **models** (```Linear Regression```, ```Decision Tree Regressor```, ```Random Forest Regressor```, \n",
    "    ```Lasso```, ```Elastic Net```, ```Ridge```, ```Support Vector Regression (linear)```, \n",
    "    ```Support Vector Regression (rbf)```, ```Support Vector Regression (poly)```)\n",
    "      * 6 **metrics**\n",
    "\n",
    "In order to handle the different setups effectively therefore the following classes will be used\n",
    "\n",
    "* ```DataSet``` representing the two data sets _day_ and _hour_\n",
    "* ```Variant``` representing different variants two be compared (see chapter _Hypotheses_)\n",
    "* ```DataSplit``` storing x_train, x_test, y_train, y_test of a train/test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load data\n",
    "\n",
    "```TIP FOR DEVS``` Put column names into variables in order to benefit from code completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Original columns\n",
    "col_instant = 'instant'\n",
    "col_datetime = 'datetime'\n",
    "col_season = 'season'\n",
    "col_year = 'year'\n",
    "col_month = 'month'\n",
    "col_hour = 'hour'\n",
    "col_holiday = 'holiday'\n",
    "col_weekday = 'weekday'\n",
    "col_workingday = 'workingday'\n",
    "col_weather_situation = 'weather_situation'\n",
    "col_temperature = 'temperature'\n",
    "col_apparent_temperature = 'apparent_temperature'\n",
    "col_humidity = 'humidity'\n",
    "col_windspeed = 'windspeed'\n",
    "\n",
    "# Historic columns\n",
    "col_weather_situation_hist = 'weather_situation_hist'\n",
    "col_temperature_hist = 'temperature_hist'\n",
    "col_apparent_temperature_hist = 'apparent_temperature_hist'\n",
    "col_humidity_hist = 'humidity_hist'\n",
    "col_windspeed_hist = 'windspeed_hist'\n",
    "\n",
    "# Target columns\n",
    "col_casual = 'casual'\n",
    "col_registered = 'registered'\n",
    "col_count = 'count'\n",
    "\n",
    "# Calculated columns\n",
    "col_days_since_start = 'days_since_start'\n",
    "\n",
    "# Raw column\n",
    "col_temperature_raw = 'temperature_raw'\n",
    "col_apparent_temperature_raw = 'apparent_temperature_raw'\n",
    "col_humidity_raw = 'humidity_raw'\n",
    "col_windspeed_raw = 'windspeed_raw'\n",
    "col_temperature_raw_rounded = 'temperature_raw_rounded'\n",
    "col_days_since_start_raw = 'days_since_start_raw'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data sets\n",
    "data_set_day = \"day\"\n",
    "data_set_hour = \"hour\"\n",
    "\n",
    "class DataSet:\n",
    "    \"\"\"Contains data, model and predictions of a data set\"\"\"\n",
    "    file_name = \"\"\n",
    "    attribute_names = []\n",
    "    columns_categorical = []\n",
    "    data_frame = {}\n",
    "    variants = {}\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "\n",
    "# Define data sets\n",
    "data_sets = {\n",
    "    # data_set_day: DataSet(\"../data/bike-sharing-dataset/day.csv\"),\n",
    "    # data_set_hour: DataSet(\"../data/bike-sharing-dataset/hour.csv\")\n",
    "    data_set_day: DataSet(\"https://raw.githubusercontent.com/florianschwanz/fom-applied-programming-challenge/master/data/bike-sharing-dataset/day.csv\"),\n",
    "    data_set_hour: DataSet(\"https://raw.githubusercontent.com/florianschwanz/fom-applied-programming-challenge/master/data/bike-sharing-dataset/hour.csv\")\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define attribute names\n",
    "data_sets[data_set_day].attribute_names = [col_instant, col_datetime, col_season, col_year, col_month, col_holiday, \n",
    "                                           col_weekday, col_workingday, col_weather_situation, col_temperature, \n",
    "                                           col_apparent_temperature, col_humidity, col_windspeed, col_casual, \n",
    "                                           col_registered, col_count]\n",
    "data_sets[data_set_hour].attribute_names = [col_instant, col_datetime, col_season, col_year, col_month, col_hour, \n",
    "                                            col_holiday, col_weekday, col_workingday, col_weather_situation, \n",
    "                                            col_temperature, col_apparent_temperature, col_humidity, col_windspeed, col_casual, col_registered, col_count]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Read csv file\n",
    "    data_set.data_frame = pd.read_csv(data_set.file_name, skiprows=1, names=data_set.attribute_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Understand data\n",
    "\n",
    "### 2.1 Show basic facts\n",
    "\n",
    "* Show data types of features\n",
    "* Make sure there are no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    data_set.data_frame.info()\n",
    "    data_set.data_frame.isnull().sum()\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Set data types\n",
    "\n",
    "Data types are assigned to features in order to make it easier to use them e.g. for calculations in further steps.\n",
    "This includes\n",
    "* date time data\n",
    "* categorical data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define categorical attributes\n",
    "data_sets[data_set_day].columns_categorical = [col_season, col_year, col_month, col_holiday, col_weekday, \n",
    "                                               col_workingday, col_weather_situation]\n",
    "data_sets[data_set_hour].columns_categorical = [col_season, col_year, col_month, col_hour, col_holiday, col_weekday, \n",
    "                                                col_workingday, col_weather_situation]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Convert columns to date time\n",
    "    data_set.data_frame[col_datetime] = pd.to_datetime(data_set.data_frame[col_datetime])\n",
    "\n",
    "    # Convert columns to category\n",
    "    for column in data_set.columns_categorical:\n",
    "        data_set.data_frame[column]=data_set.data_frame[column].astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Restore raw data\n",
    "\n",
    "* Some features in the data sets are already normalized\n",
    "* For better visualization they need to be de-normalized first (using extreme values from data set's readme file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Extreme temperature values\n",
    "temperature_min=-8\n",
    "temperature_max=39\n",
    "# Extreme apparent temperature values\n",
    "apparent_temperature_min=-16\n",
    "apparent_temperature_max=50\n",
    "# Extreme humidity value\n",
    "humidity_max=100\n",
    "# Extreme wind speed value\n",
    "windspeed_max=67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Define columns containing raw-values\n",
    "columns_raw_values = [col_temperature_raw, col_apparent_temperature_raw, col_humidity_raw, col_windspeed_raw, \n",
    "                      col_temperature_raw_rounded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Restore raw values for day data frame\n",
    "    data_set.data_frame = data_set.data_frame.assign(temperature_raw=data_set.data_frame[col_temperature] * (temperature_max-temperature_min) + temperature_min)\n",
    "    data_set.data_frame = data_set.data_frame.assign(apparent_temperature_raw=data_set.data_frame[col_apparent_temperature] * (apparent_temperature_max-apparent_temperature_min) + apparent_temperature_min)\n",
    "    data_set.data_frame = data_set.data_frame.assign(humidity_raw=data_set.data_frame[col_humidity] * humidity_max)\n",
    "    data_set.data_frame = data_set.data_frame.assign(windspeed_raw=data_set.data_frame[col_windspeed] * windspeed_max)\n",
    "    \n",
    "    # Round values for better visualization\n",
    "    data_set.data_frame = data_set.data_frame.assign(temperature_raw_rounded=round(data_set.data_frame[col_temperature_raw]/5,0)*5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Show data\n",
    "\n",
    "```WARNING``` DataFrame.head() does not work inside loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Show first few lines\n",
    "    data_set.data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_day].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_hour].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.5 Plot relation between month and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot contains few peaks (most popular biking months)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_month,col_count,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='point', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.6 Plot relation between temperature and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot contains one peak (optimal biking temperature)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_temperature_raw_rounded,col_count,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='point', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.7 Plot relation between weather situation and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot indicates that there are significantly more rentals on day having good weather (clear or misty) \n",
    "  * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "  * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "  * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "  * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_weather_situation,col_count,hue=col_year,data=data_sets[data_set_day].data_frame, ci=None, kind='strip', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.8 Plot relation between weather situation and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot indicates that more bikes are rented during day time\n",
    "  * Probably there are two peeks when people go to work / go back home"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_hour,col_count,hue=col_year,data=data_sets[data_set_hour].data_frame, ci=None, kind='point', palette='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.9 Plot correlation matrix of some attributes\n",
    "\n",
    "* Expected matrix contains the following correlations\n",
    "  * strong correlation between _temperature_ and _apparent temperature_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    correlationMatrix=data_set.data_frame[[col_temperature, col_apparent_temperature, col_humidity,col_windspeed,\n",
    "                                           col_casual, col_registered]].corr()\n",
    "    mask=np.array(correlationMatrix)\n",
    "    mask[np.tril_indices_from(mask)]=False\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(15,8))\n",
    "    sns.heatmap(correlationMatrix,mask=mask,vmax=0.8,square=True,annot=True,ax=ax)\n",
    "    ax.set_title('Correlation Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Hypotheses\n",
    "\n",
    "Based on the basic understanding of the data sets two hypotheses shall be made\n",
    "\n",
    "In the course of this notebook the data sets will be extended by additional information\n",
    "* days since start of the bike sharing campaign\n",
    "* weather conditions of previous period\n",
    "\n",
    "### 3.1 Hypothesis A\n",
    "\n",
    "There is a significant difference in the number of rented bikes in the two years being contained in the data set. \n",
    "Therefore it can be assumed that the raising popularity correlates with the number of rented bikes.\n",
    "In order to include this assumption a synthentic feature ```days since start``` shall be used.\n",
    "\n",
    "*Hypothesis* Taking into account the number of days since the bike sharing campaign has started will increase the \n",
    "performance of the model.\n",
    "\n",
    "### 3.2 Hypothesis B\n",
    "\n",
    "Different aspects of the weather affect how many bikes are rented. In most cases there may be time between _deciding to \n",
    "use a shared bike_ and _actually using it_. Therefore also the weather conditions of the time period \n",
    "_before_ a bike is rented may be relevant. In order to achieve this some synthetic features such as \n",
    "```weather_situation_hist```, ```temperature_hist```, ```apparent_temperature_hist```, ```humidity_hist```, \n",
    "```windspeed_hist``` shall be included which reflect the weather situation of the previous row.\n",
    "\n",
    "*Hypothesis* Taking into account the weather conditions of the previous period will increase the \n",
    "performance of the model.\n",
    "\n",
    "### 3.3 Hypothesis C\n",
    "\n",
    "Features ```temperature``` and ```apparent_temperature``` strongly correlate with each other. It is to be assumed that \n",
    "dropping one of the two does not reduce model performance.\n",
    "\n",
    "*Hypothesis* Dropping _apparent temperature_ does not decrease performance of the model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Initialize variants\n",
    "\n",
    "Three variants will be trained and evaluated\n",
    "* ```original``` original data frame\n",
    "* ```market``` original data frame + days since start\n",
    "* ```history``` original data frame + history weather conditions\n",
    "* ```reduced``` original data frame + manual dimensionality reduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Variants\n",
    "variant_original = \"original\"\n",
    "variant_market = \"market\"\n",
    "variant_history = \"history\"\n",
    "variant_reduced = \"reduced\"\n",
    "\n",
    "class Variant:\n",
    "    \"\"\"Contains data, model and predictions of a variant\"\"\"\n",
    "    color = \"#000\"\n",
    "    data_frame = {}\n",
    "    folds = {}\n",
    "    models = {}\n",
    "    \n",
    "    def __init__(self, color):\n",
    "        self.color = color"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Assign variants to data frames\n",
    "    data_set.variants = {\n",
    "        variant_original: Variant(color=\"#d4dd80\"),\n",
    "        variant_market: Variant(color=\"#3fe0e0\"),\n",
    "        variant_history: Variant(color=\"#3fb5e0\"),\n",
    "        variant_reduced: Variant(color=\"red\"),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 One-hot-encode categorical data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # One-hot encode columns \n",
    "    for column in data_set.columns_categorical:\n",
    "        data_set.data_frame = pd.concat([data_set.data_frame,pd.get_dummies(data_set.data_frame[column], prefix=column)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Enhance data\n",
    "\n",
    "* The following variants shall be compared\n",
    "  * _original_ original features\n",
    "  * _market_ original features + time since start of bike sharing campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        # Copy original data frame\n",
    "        variant.data_frame = data_set.data_frame.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3.1 Variant _market_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Calculate days since start\n",
    "    start_date = data_set.variants[variant_market].data_frame[col_datetime].min()\n",
    "    data_set.variants[variant_market].data_frame[col_days_since_start_raw] = \\\n",
    "        data_set.variants[variant_market].data_frame.apply(lambda row: (row[col_datetime] - start_date).days, axis = 1) \n",
    "    \n",
    "    # Normalize days since start\n",
    "    max_days_since_start = data_set.variants[variant_market].data_frame[col_days_since_start_raw].max()\n",
    "    data_set.variants[variant_market].data_frame[col_days_since_start] = \\\n",
    "        data_set.variants[variant_market].data_frame.apply(lambda row: (row[col_days_since_start_raw] / max_days_since_start), axis = 1)\n",
    "    \n",
    "    # Drop raw days since start\n",
    "    data_set.variants[variant_market].data_frame.drop(col_days_since_start_raw, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_day].variants[variant_market].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    correlationMatrix=data_set.variants[variant_market].data_frame[[col_temperature, col_apparent_temperature, \n",
    "                                                                    col_humidity, col_windspeed, col_days_since_start, \n",
    "                                                                    col_count]].corr()\n",
    "    mask=np.array(correlationMatrix)\n",
    "    mask[np.tril_indices_from(mask)]=False\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(15,8))\n",
    "    sns.heatmap(correlationMatrix,mask=mask,vmax=0.8,square=True,annot=True,ax=ax)\n",
    "    ax.set_title('Correlation Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3.2 Variant _history_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    historical_period = 1\n",
    "    \n",
    "    # Copy columns into hist columns and shift them by X\n",
    "    data_set.variants[variant_history].data_frame[col_weather_situation_hist] = data_set.variants[variant_history].data_frame[col_weather_situation].shift(historical_period)\n",
    "    data_set.variants[variant_history].data_frame[col_temperature_hist] = data_set.variants[variant_history].data_frame[col_temperature].shift(historical_period)\n",
    "    data_set.variants[variant_history].data_frame[col_apparent_temperature_hist] = data_set.variants[variant_history].data_frame[col_apparent_temperature].shift(historical_period)\n",
    "    data_set.variants[variant_history].data_frame[col_humidity_hist] = data_set.variants[variant_history].data_frame[col_humidity].shift(historical_period)\n",
    "    data_set.variants[variant_history].data_frame[col_windspeed_hist] = data_set.variants[variant_history].data_frame[col_windspeed].shift(historical_period)\n",
    "    \n",
    "    # Remove row historical_period\n",
    "    data_set.variants[variant_history].data_frame = data_set.variants[variant_history].data_frame.iloc[historical_period:]\n",
    "    \n",
    "    # One-hot encode columns \n",
    "    data_set.variants[variant_history].data_frame = pd.concat([data_set.variants[variant_history].data_frame, pd.get_dummies(data_set.variants[variant_history]\n",
    "                                                                              .data_frame[col_weather_situation_hist], prefix=col_weather_situation_hist)],axis=1)\n",
    "    # Drop one-hot encoded columns\n",
    "    data_set.variants[variant_history].data_frame.drop([col_weather_situation_hist], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_day].variants[variant_history].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3.3 Variant _reduced_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # data_set.variants[variant_reduced].data_frame.drop(list(data_set.variants[variant_reduced].data_frame.filter(regex=col_season + \"_\")), axis=1, inplace=True)\n",
    "    data_set.variants[variant_reduced].data_frame.drop(list(data_set.variants[variant_reduced].data_frame.filter(regex=\"^\" + col_apparent_temperature + \"$\")), axis=1, inplace=True)\n",
    "    # data_set.variants[variant_reduced].data_frame.drop(list(data_set.variants[variant_reduced].data_frame.filter(regex=col_weekday + \"_\")), axis=1, inplace=True)\n",
    "    # data_set.variants[variant_reduced].data_frame.drop(list(data_set.variants[variant_reduced].data_frame.filter(regex=col_month + \"_\")), axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_sets[data_set_day].variants[variant_reduced].data_frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 Normalize data\n",
    "\n",
    "Nothing to be done here since features are already normalized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5 Drop columns\n",
    "\n",
    "* The following columns need to be dropped\n",
    "  * Indices\n",
    "    * _instant_ since it does not contain any information besides the order\n",
    "  * Columns which cannot be used directly\n",
    "    * _datetime_ which is formatted ```yyyy-mm-dd```\n",
    "  * Columns which are one-hot encoded\n",
    "  * Columns which contain raw values\n",
    "  * Unused target columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        # Drop index and unusable columns\n",
    "        variant.data_frame.drop([col_instant, col_datetime], axis=1, inplace=True)\n",
    "        # Drop one-hot encoded columns\n",
    "        variant.data_frame.drop(data_set.columns_categorical, axis=1, inplace=True)\n",
    "        # Drop raw-value columns\n",
    "        variant.data_frame.drop(columns_raw_values, axis=1, inplace=True)\n",
    "        # Drop unused target columns    \n",
    "        variant.data_frame.drop([col_casual, col_registered], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Split data into _training_ and _test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "    \"\"\"Contains results of a train/test data split\"\"\"\n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "\n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "        \n",
    "        # Define columns\n",
    "        data_bike_x = variant.data_frame.drop([col_count], axis=1)\n",
    "        data_bike_y = variant.data_frame[[col_count]]\n",
    "        \n",
    "        n_splits = 10\n",
    "        random_state = 0\n",
    "        \n",
    "        # Initialize KFold\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        \n",
    "        fold_index = 0\n",
    "        variant.folds = {}\n",
    "        \n",
    "        for train_index, test_index in kf.split(data_bike_x):\n",
    "        \n",
    "            x_train, x_test = data_bike_x.iloc[train_index], data_bike_x.iloc[test_index]\n",
    "            y_train, y_test = data_bike_y.iloc[train_index], data_bike_y.iloc[test_index]\n",
    "            \n",
    "            # Add splitted data \n",
    "            variant.folds[fold_index] = DataSplit(x_train, x_test, y_train, y_test)  \n",
    "            \n",
    "            # Show splitted data\n",
    "            # print(\"data set \" + data_set_name + \" / variant \" + variant_name + \" / \" + str(x_train.shape) + \", \" + str(x_test.shape) + \", \" + str(y_train.shape) + \", \" + str(y_test.shape))\n",
    "\n",
    "            fold_index += 1\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6 Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Put models into dictionary\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    # \"Decision Tree Regressor (25)\": DecisionTreeRegressor(min_samples_split=10, max_leaf_nodes=25),\n",
    "    # \"Decision Tree Regressor (50)\": DecisionTreeRegressor(min_samples_split=10, max_leaf_nodes=50),\n",
    "    # \"Decision Tree Regressor (100)\": DecisionTreeRegressor(min_samples_split=10, max_leaf_nodes=100),\n",
    "    \"Decision Tree Regressor (200)\": DecisionTreeRegressor(min_samples_split=10, max_leaf_nodes=200),\n",
    "    # \"Random Forest Regressor (25)\": RandomForestRegressor(n_estimators=25),\n",
    "    # \"Random Forest Regressor (50)\": RandomForestRegressor(n_estimators=50),\n",
    "    # \"Random Forest Regressor (100)\": RandomForestRegressor(n_estimators=100),\n",
    "    \"Random Forest Regressor (200)\": RandomForestRegressor(n_estimators=200),\n",
    "    # \"Lasso (0.2)\": Lasso(alpha=0.2),\n",
    "    # \"Lasso (0.4)\": Lasso(alpha=0.4),\n",
    "    # \"Lasso (0.6)\": Lasso(alpha=0.6),\n",
    "    # \"Lasso (0.8)\": Lasso(alpha=0.8),\n",
    "    \"Lasso (1.0)\": Lasso(alpha=1.0),\n",
    "    \"Elastic Net (0.2)\": ElasticNet(alpha=0.2, random_state=0),\n",
    "    \"Elastic Net (0.4)\": ElasticNet(alpha=0.4, random_state=0),\n",
    "    # \"Elastic Net (0.6)\": ElasticNet(alpha=0.6, random_state=0),\n",
    "    # \"Elastic Net (0.8)\": ElasticNet(alpha=0.8, random_state=0),\n",
    "    # \"Elastic Net (1.0)\": ElasticNet(alpha=1.0, random_state=0),\n",
    "    # \"Ridge (0.2)\": Ridge(alpha=0.2),\n",
    "    # \"Ridge (0.4)\": Ridge(alpha=0.4),\n",
    "    # \"Ridge (0.6)\": Ridge(alpha=0.6),\n",
    "    # \"Ridge (0.8)\": Ridge(alpha=0.8),\n",
    "    \"Ridge (1.0)\": Ridge(alpha=1.0),\n",
    "    # \"Support Vector Regression (linear)\": SVR(kernel='linear', gamma='scale', C=1.0, epsilon=0.2),\n",
    "    # \"Support Vector Regression (rbf)\": SVR(kernel='rbf', gamma='scale', C=1.0, epsilon=0.2),\n",
    "    # \"Support Vector Regression (poly)\": SVR(kernel='poly', gamma='scale', C=1.0, epsilon=0.2),\n",
    "    # \"Support Vector Regression (sigmoid)\": SVR(kernel='sigmoid', gamma='scale', C=1.0, epsilon=0.2),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   },
   "source": [
    "# Iterate over data sets\n",
    "for data_set_name, data_set in data_sets.items():\n",
    "    \n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "    \n",
    "        # Iterate over models\n",
    "        for model_name, model in models.items():\n",
    "            \n",
    "            # Attach model to variant\n",
    "            variant.models[model_name] = model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trainAndEvaluateDataSet(data_set_name, data_set):\n",
    "    \"\"\"Trains and evaluates a given data set\"\"\"\n",
    "\n",
    "    variant_index = 0\n",
    "    y_pos = np.arange(len(models.keys()))\n",
    "    handles = []\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    # Iterate over variants\n",
    "    for variant_name, variant in data_set.variants.items():\n",
    "\n",
    "        print(variant_name)\n",
    "\n",
    "        model_index = 0\n",
    "        metric_root_mean_squared_error_list = []\n",
    "        metric_r2_score_list = []\n",
    "\n",
    "        # Iterate over models in variant\n",
    "        for model_name, model in variant.models.items():\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Initialize metric\n",
    "            metric_explained_variance_score = 0\n",
    "            metric_max_error = 0\n",
    "            metric_mean_absolute_error = 0\n",
    "            metric_mean_squared_error = 0\n",
    "            metric_root_mean_squared_error = 0\n",
    "            metric_median_absolute_error = 0\n",
    "            metric_r2_score = 0\n",
    "\n",
    "            # Iterate over folds in variant\n",
    "            for fold_name, fold in variant.folds.items():\n",
    "                \n",
    "                # Fit model and create prediction\n",
    "                model.fit(fold.x_train, fold.y_train)\n",
    "\n",
    "                # Make prediction\n",
    "                predicted_values = model.predict(fold.x_test)\n",
    "                expected_values = fold.y_test\n",
    "\n",
    "                # Calculate scores\n",
    "                metric_explained_variance_score += explained_variance_score(expected_values, predicted_values)\n",
    "                metric_max_error += max_error(expected_values, predicted_values)\n",
    "                metric_mean_absolute_error += mean_absolute_error(expected_values, predicted_values)\n",
    "                metric_mean_squared_error += mean_squared_error(expected_values, predicted_values)\n",
    "                metric_root_mean_squared_error += sqrt(mean_squared_error(expected_values, predicted_values))\n",
    "                metric_median_absolute_error += median_absolute_error(expected_values, predicted_values)\n",
    "                metric_r2_score += r2_score(expected_values, predicted_values)\n",
    "\n",
    "                # print(\"explained variance score\", round(metric_explained_variance_score, 2))\n",
    "                # print(\"               max error\", round(metric_max_error, 2))\n",
    "                # print(\"     mean absolute error\", round(metric_mean_absolute_error, 2))\n",
    "                # print(\"      mean squared error\", round(metric_mean_squared_error, 2))\n",
    "                # print(\"   median absolute error\", round(metric_median_absolute_error, 2))\n",
    "                # print(\"                r2 score\", round(metric_r2_score, 2))\n",
    "\n",
    "            fold_count = len(variant.folds.items())\n",
    "\n",
    "            # Divide scores by number of folds\n",
    "            metric_explained_variance_score /= fold_count\n",
    "            metric_max_error /= fold_count\n",
    "            metric_mean_absolute_error /= fold_count\n",
    "            metric_mean_squared_error /= fold_count\n",
    "            metric_root_mean_squared_error /= fold_count\n",
    "            metric_median_absolute_error /= fold_count\n",
    "            metric_r2_score /= fold_count\n",
    "\n",
    "            end = time.time()\n",
    "            # print(\"  (\" + str(round(end-start, 2)) + \"s)\\t\" + model_name + \" > \" + str(round(metric_root_mean_squared_error, 2)))\n",
    "            print(\"  (\" + str(round(end-start, 2)) + \"s)\\t\" + model_name + \" > \" + str(round(metric_r2_score, 2)))\n",
    "\n",
    "            metric_root_mean_squared_error_list.insert(model_index, metric_root_mean_squared_error)\n",
    "            metric_r2_score_list.insert(model_index, metric_r2_score)\n",
    "            model_index += 1\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "        handle = plt.barh(y_pos + (0.8 * variant_index / len(data_set.variants)), metric_r2_score_list, 0.8 * 1/len(data_set.variants), align='center', alpha=0.5, color=variant.color, label=variant_name)\n",
    "        handles.insert(variant_index, handle)\n",
    "        variant_index += 1\n",
    "\n",
    "    plt.xlabel('R2 Score')\n",
    "    plt.yticks(y_pos, models.keys())\n",
    "    plt.title('Model performance')\n",
    "    plt.legend(handles=handles)\n",
    "    plt.show()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.1 Data Set _day_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and evaluate data set 'day'\n",
    "%time trainAndEvaluateDataSet(data_set_day, data_sets[data_set_day])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2 Data Set _hour_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and evaluate data set 'hour'\n",
    "%time trainAndEvaluateDataSet(data_set_hour, data_sets[data_set_hour])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8 Conclusion\n",
    "\n",
    "### 3.1 Hypothesis A\n",
    "\n",
    "In ```day``` data set using synthetic feature _days since start_ did not improve the performance when using\n",
    "* Linear Regression (0.83 > 0.83)\n",
    "* Decision Tree Regressor (200) (0.80 > 0.80)\n",
    "* Lasso (1.0) (0.83 > 0.83)\n",
    "* Elastic Net (0.2) (0.75 > 0.75)\n",
    "* Elastic Net (0.4) (0.69 > 0.69)\n",
    "* Ridge (1.0) (0.83 > 0.83)\n",
    "\n",
    "and slightly improves the performance when using\n",
    "* Random Forest Regressor (200) (0.87 > 0.88)\n",
    "\n",
    "### 3.2 Hypothesis B\n",
    "\n",
    "In ```day``` data set using historic data slightly increased performance when using\n",
    "\n",
    "* Lasso (1.0) (0.83 > 0.84)\n",
    "* Elastic Net (0.2) (0.75 > 0.76)\n",
    "* Elastic Net (0.4) (0.69 > 0.70)\n",
    "* Ridge (1.0) (0.83 > 0.84)\n",
    "\n",
    "... did not affect the performance when using\n",
    "* Linear Regression (0.83 > 0.83)\n",
    "* Random Forest Regressor (200) (0.87 > 0.87)\n",
    "\n",
    "... and decreases the performance when using\n",
    "* Decision Tree Regressor (200) (0.80 > 0.76)\n",
    "\n",
    "### 3.3 Hypothesis C\n",
    "\n",
    "In ```day``` data set manually reducing the dimensionality by removing _apparent temperature_ slightly decreases the \n",
    "performance when using\n",
    "* Linear Regression (200) (0.83 > 0.81)\n",
    "* Decision Tree Regressor (200) (0.80 > 0.76)\n",
    "* Random Forest Regressor (0.87 > 0.86)\n",
    "* Lasso (1.0) (0.83 > 0.81)\n",
    "* Elastic Net (0.2) (0.75 > 0.67)\n",
    "* Elastic Net (0.4) (0.69 > 0.57) and\n",
    "* Rigde (1.0) (0.83 > 0.81)\n",
    "\n",
    "There is no model which increased its performance compared to the original data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}