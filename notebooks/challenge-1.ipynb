{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Applied Programming Coding Challenge #1\n",
    "\n",
    "![title](../img/photo-1533788179956-82e8a027c962.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Original columns\n",
    "col_instant='instant'\n",
    "col_datetime='datetime'\n",
    "col_season='season'\n",
    "col_year='year'\n",
    "col_month='month'\n",
    "col_hour='hour'\n",
    "col_holiday='holiday'\n",
    "col_weekday='weekday'\n",
    "col_workingday='workingday'\n",
    "col_weather_situation='weather_situation'\n",
    "col_temperature='temperature'\n",
    "col_apparent_temperature='apparent_temperature'\n",
    "col_humidity='humidity'\n",
    "col_windspeed='windspeed'\n",
    "\n",
    "# Target columns\n",
    "col_casual='casual'\n",
    "col_registered='registered'\n",
    "col_count='count'\n",
    "\n",
    "# Calculated columns\n",
    "col_days_since_start='days_since_start'\n",
    "\n",
    "# Raw column\n",
    "col_temperature_raw='temperature_raw'\n",
    "col_apparent_temperature_raw='apparent_temperature_raw'\n",
    "col_humidity_raw='humidity_raw'\n",
    "col_windspeed_raw='windspeed_raw'\n",
    "col_temperature_raw_rounded='temperature_raw_rounded'\n",
    "col_days_since_start_raw='days_since_start_raw'\n",
    "\n",
    "# Variants\n",
    "variant_original = \"original\"\n",
    "variant_market = \"market\"\n",
    "\n",
    "# Define attribute names\n",
    "attribute_names_day = [col_instant, col_datetime, col_season, col_year, col_month, col_holiday, col_weekday, \n",
    "                       col_workingday, col_weather_situation, col_temperature, col_apparent_temperature, col_humidity, \n",
    "                       col_windspeed, col_casual, col_registered, col_count]\n",
    "\n",
    "# Read csv files\n",
    "data_bike_day = pd.read_csv(\"../data/bike-sharing-dataset/day.csv\", skiprows=1, names=attribute_names_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Understand data\n",
    "\n",
    "### 2.1 Show basic facts\n",
    "\n",
    "* Show data types of features\n",
    "* Make sure there are no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_bike_day.info()\n",
    "data_bike_day.isnull().sum()\n",
    "# data_bike_day.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert columns to date time\n",
    "data_bike_day[col_datetime] = pd.to_datetime(data_bike_day[col_datetime])\n",
    "\n",
    "# Define categorical columns\n",
    "columns_categorical = [col_season, col_year, col_month, col_holiday, col_weekday, col_workingday, col_weather_situation]\n",
    "\n",
    "# Convert columns to category\n",
    "for column in columns_categorical:\n",
    "    data_bike_day[column]=data_bike_day[column].astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extreme temperature values\n",
    "temperature_min=-8\n",
    "temperature_max=39\n",
    "# Extreme apparent temperature values\n",
    "apparent_temperature_min=-16\n",
    "apparent_temperature_max=50\n",
    "# Extreme humidity value\n",
    "humidity_max=100\n",
    "# Extreme wind speed value\n",
    "windspeed_max=67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define columns containing raw-values\n",
    "columns_raw_values = [col_temperature_raw, col_apparent_temperature_raw, col_humidity_raw, col_windspeed_raw, \n",
    "                      col_temperature_raw_rounded]\n",
    "\n",
    "# Restore raw values for day data frame\n",
    "data_bike_day = data_bike_day.assign(temperature_raw=data_bike_day[col_temperature] * (temperature_max-temperature_min) + temperature_min)\n",
    "data_bike_day = data_bike_day.assign(apparent_temperature_raw=data_bike_day[col_apparent_temperature] * (apparent_temperature_max-apparent_temperature_min) + apparent_temperature_min)\n",
    "data_bike_day = data_bike_day.assign(humidity_raw=data_bike_day[col_humidity] * humidity_max)\n",
    "data_bike_day = data_bike_day.assign(windspeed_raw=data_bike_day[col_windspeed] * windspeed_max)\n",
    "\n",
    "# Round values for better visualization\n",
    "data_bike_day = data_bike_day.assign(temperature_raw_rounded=round(data_bike_day[col_temperature_raw]/5,0)*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show first few lines\n",
    "data_bike_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Plot relation between month and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot contains few peaks (most popular biking months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_month,col_count,hue=col_year,data=data_bike_day, ci=None, kind='point', palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Plot relation between temperature and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot contains one peak (optimal biking temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_temperature_raw_rounded,col_count,hue=col_year,data=data_bike_day, ci=None, kind='point', palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Plot relation between weather situation and number of rented bikes\n",
    "\n",
    "* Make sure data is plausible\n",
    "* Expected plot indicates that there are significantly more rentals on day having good weather (clear or misty) \n",
    "  * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "  * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "  * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "  * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot trend\n",
    "sns.catplot(col_weather_situation,col_count,hue=col_year,data=data_bike_day, ci=None, kind='strip', palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Plot correlation matrix of some attributes\n",
    "\n",
    "* Expected matrix contains the following correlations\n",
    "  * strong correlation between _temperature_ and _apparent temperature_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "correlationMatrix=data_bike_day[[col_temperature,col_apparent_temperature,col_humidity,col_windspeed,col_casual,col_registered]].corr()\n",
    "mask=np.array(correlationMatrix)\n",
    "mask[np.tril_indices_from(mask)]=False\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(15,8))\n",
    "sns.heatmap(correlationMatrix,mask=mask,vmax=0.8,square=True,annot=True,ax=ax)\n",
    "ax.set_title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.0 Initialize variants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Variant:\n",
    "    \"\"\"Contains data, model and predictions of a variant\"\"\"\n",
    "    color = \"#000\"\n",
    "    data_frame = {}\n",
    "    splitted_data = {}\n",
    "    models = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    def __init__(self, color):\n",
    "        self.color = color\n",
    "\n",
    "# Define variants\n",
    "variants = {\n",
    "    variant_original: Variant(color=\"#3fe0e0\"),\n",
    "    variant_market: Variant(color=\"#d4dd80\"),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 One-hot-encode categorical data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode columns \n",
    "for column in columns_categorical:\n",
    "    data_bike_day = pd.concat([data_bike_day,pd.get_dummies(data_bike_day[column], prefix=column)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Enhance data\n",
    "\n",
    "* The following variants shall be compared\n",
    "  * _original_ original features\n",
    "  * _market_ original features + time since start of bike sharing campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over variants\n",
    "for variant_name, variant in variants.items():\n",
    "    \n",
    "    # Copy original data frame\n",
    "    variant.data_frame = data_bike_day.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.1 Calculate date since start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate days since start\n",
    "start_date = variants[variant_market].data_frame[col_datetime].min()\n",
    "variants[variant_market].data_frame[col_days_since_start_raw] = \\\n",
    "    variants[variant_market].data_frame.apply(lambda row: (row[col_datetime] - start_date).days, axis = 1) \n",
    "\n",
    "# Normalize days since start\n",
    "max_days_since_start = variants[variant_market].data_frame[col_days_since_start_raw].max()\n",
    "variants[variant_market].data_frame[col_days_since_start] = \\\n",
    "    variants[variant_market].data_frame.apply(lambda row: (row[col_days_since_start_raw] / max_days_since_start), axis = 1)\n",
    "\n",
    "# Drop raw days since start\n",
    "variants[variant_market].data_frame.drop(col_days_since_start_raw, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "correlationMatrix=variants[variant_market].data_frame[[col_temperature,col_apparent_temperature,col_humidity,col_windspeed,col_days_since_start,col_count]].corr()\n",
    "mask=np.array(correlationMatrix)\n",
    "mask[np.tril_indices_from(mask)]=False\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(15,8))\n",
    "sns.heatmap(correlationMatrix,mask=mask,vmax=0.8,square=True,annot=True,ax=ax)\n",
    "ax.set_title('Correlation Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Normalize data\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Drop columns\n",
    "\n",
    "* The following columns need to be dropped\n",
    "  * Indices\n",
    "    * _instant_ since it does not contain any information besides the order\n",
    "  * Columns which cannot be used directly\n",
    "    * _datetime_ which is formatted ```yyyy-mm-dd```\n",
    "  * Columns which are one-hot encoded\n",
    "  * Columns which contain raw values\n",
    "  * Unused target columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over variants\n",
    "for variant_name, variant in variants.items():\n",
    "    \n",
    "    # Drop index and unusable columns\n",
    "    variant.data_frame.drop([col_instant, col_datetime, col_temperature], axis=1, inplace=True)\n",
    "    # Drop one-hot encoded columns\n",
    "    variant.data_frame.drop(columns_categorical, axis=1, inplace=True)\n",
    "    # Drop raw-value columns\n",
    "    variant.data_frame.drop(columns_raw_values, axis=1, inplace=True)\n",
    "    # Drop unused target columns    \n",
    "    variant.data_frame.drop([col_casual, col_registered], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Split data into _training_ and _test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "    \"\"\"Contains results of a train/test data split\"\"\"\n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "# Iterate over variants\n",
    "for variant_name, variant in variants.items():\n",
    "    \n",
    "    # Define columns\n",
    "    data_bike_day_x = variant.data_frame.drop([col_count], axis=1)\n",
    "    data_bike_day_y = variant.data_frame[[col_count]]\n",
    "    \n",
    "    test_size = 0.3\n",
    "    random_state = 0\n",
    "    \n",
    "    # Split training data and test data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_bike_day_x, data_bike_day_y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Add splitted data \n",
    "    variant.splitted_data = DataSplit(x_train, x_test, y_train, y_test)\n",
    "    \n",
    "    # Show splitted data\n",
    "    print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5 Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Put models into dictionary\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(min_samples_split=10,max_leaf_nodes=100),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=200),\n",
    "    \"Lasso\": Lasso(alpha=0.2),\n",
    "    \"Elastic Net\": ElasticNet(alpha=0.2, random_state=0),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Support Vector Regression (linear)\": SVR(kernel='linear', gamma='scale', C=1.0, epsilon=0.2),\n",
    "    \"Support Vector Regression (rbf)\": SVR(kernel='rbf', gamma='scale', C=1.0, epsilon=0.2)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "source": [
    "# Iterate over variants\n",
    "for variant_name, variant in variants.items():\n",
    "\n",
    "    # Iterate over models\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        # Attach model to variant\n",
    "        variant.models[model_name] = model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over variants\n",
    "for variant_name, variant in variants.items():\n",
    "    \n",
    "    # Iterate over models in variant \n",
    "    for model_name, model in variant.models.items():\n",
    "        \n",
    "        # Fit model and create prediction\n",
    "        model.fit(variant.splitted_data.x_train, variant.splitted_data.y_train)\n",
    "        variant.predictions[model_name] = model.predict(variant.splitted_data.x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7 Evaluate model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluateModel(predicted_values, expected_values):\n",
    "    \"\"\"Evaluates regression model by using multiple metrics\"\"\"\n",
    "    print(\"explained variance score\", round(explained_variance_score(expected_values, predicted_values), 2))\n",
    "    print(\"               max error\", round(max_error(expected_values, predicted_values), 2))\n",
    "    print(\"     mean absolute error\", round(mean_absolute_error(expected_values, predicted_values), 2))\n",
    "    print(\"      mean squared error\", round(mean_squared_error(expected_values, predicted_values), 2))\n",
    "    print(\"  mean squared log error\", round(mean_squared_log_error(expected_values, predicted_values), 2))\n",
    "    print(\"   median absolute error\", round(median_absolute_error(expected_values, predicted_values), 2))\n",
    "    print(\"                r2 score\", round(r2_score(expected_values, predicted_values), 2))\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.1 Display metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over variants\n",
    "for variant_name, variant in variants.items():\n",
    "    \n",
    "    # Iterate over models\n",
    "    for model_name, model in variant.models.items():\n",
    "        print(variant_name + \" / \" + model_name)\n",
    "        \n",
    "        # Evaluate each model in dictionary\n",
    "        predicted_values = variant.predictions[model_name]\n",
    "        expected_values = variant.splitted_data.y_test\n",
    "        evaluateModel(predicted_values, expected_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2 Visualize R2 score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pos = np.arange(len(models.keys()))\n",
    "\n",
    "variant_index = 0\n",
    "\n",
    "# Iterate over variants\n",
    "for variant_name, variant in variants.items():\n",
    "\n",
    "    models_r2_score = []\n",
    "    models_r2_score_index = 0\n",
    "    \n",
    "    # Iterate over models\n",
    "    for model_name in models.keys():\n",
    "\n",
    "        predicted_values = variant.predictions[model_name]\n",
    "        expected_values = variant.splitted_data.y_test\n",
    "\n",
    "        models_r2_score.insert(models_r2_score_index, r2_score(expected_values, predicted_values))\n",
    "        models_r2_score_index += 1\n",
    "    \n",
    "    plt.barh(y_pos + (variant_index*0.5), models_r2_score, 0.4, align='center', alpha=0.5, color=variant.color)\n",
    "    variant_index += 1\n",
    "\n",
    "plt.xlabel('R2 Score')\n",
    "plt.yticks(y_pos, models.keys())\n",
    "plt.title('Model performance')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}